{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_openai langchain langchain_community langchain_pinecone docarray pydantic==2.7.0 python-dotenv pandas tiktoken PyPDF2 fastapi uvicorn --quiet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI model\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the PDF** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Initialize PDF reader\n",
    "pdf_reader = PdfReader('knowledge_base.pdf')\n",
    "\n",
    "# Extract text from each page\n",
    "raw_text = ''\n",
    "for page in pdf_reader.pages:\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 101, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split the raw text into chunks\n",
    "texts = text_splitter.split_text(raw_text)\n",
    "\n",
    "# Convert text chunks to Document objects\n",
    "documents = [Document(page_content=text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Everyday we hear sounds from various\\nsources like humans, bir ds, bells, machines,',\n",
       "  'vehicles, televisions, radios etc. Sound is a\\nform of ener gy which pr oduces a sensation',\n",
       "  'of hearing in our ears. Ther e are also other\\nforms of energy like mechanical energy, light',\n",
       "  'energy, etc. W e have talked about mechanical\\nenergy in the pr evious chapters. Y ou have',\n",
       "  'been taught about conservation of energy,\\nwhich states that we can neither create nor'],\n",
       " list,\n",
       " 413)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5], type(texts), len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = \"agentic-rag\"\n",
    "\n",
    "# Initialize Pinecone vector store\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chaining Components for Contextual Question Answering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't answer the question, reply \"I'm unable to resolve that with my current capabilities.\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "retrieved_contexts = []\n",
    "\n",
    "# Log the retrieved context\n",
    "def log_context(inputs):\n",
    "    context_content = [input.page_content for input in inputs['context']]\n",
    "    retrieved_contexts.append(context_content)\n",
    "    return inputs\n",
    "\n",
    "# Chain to retrieve context, log, run the prompt, and parse the output\n",
    "chain = {\n",
    "    \"context\": pinecone.as_retriever(search_kwargs={\"k\": 5}), \n",
    "    \"question\": RunnablePassthrough()\n",
    "} | RunnablePassthrough(log_context) | prompt | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent for Selective Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class DomainClassifierTool:\n",
    "    def __init__(self, domain, keywords):\n",
    "        self.domain = domain\n",
    "        self.keywords = keywords\n",
    "\n",
    "    def classify_query(self, query):\n",
    "        # Convert query to lowercase for case-insensitive matching\n",
    "        query_lower = query.lower()\n",
    "        for keyword in self.keywords:\n",
    "            # Check if keyword is in the query\n",
    "            if re.search(r'\\b' + re.escape(keyword.lower()) + r'\\b', query_lower):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Keywords related to the \"Sound\" chapter\n",
    "    relevant_keywords = [\n",
    "        \"sound\", \"wave\", \"vibration\", \"frequency\", \"amplitude\", \"wavelength\",\n",
    "        \"pitch\", \"loudness\", \"echo\", \"reverberation\", \"sonic boom\", \"ultrasound\",\n",
    "        \"infrasound\", \"decibel\", \"noise pollution\", \"musical instrument\",\n",
    "        \"longitudinal wave\", \"compressions\", \"rarefactions\", \"speed of sound\",\n",
    "        \"medium\", \"reflection of sound\", \"absorption of sound\"\n",
    "    ]\n",
    "    # Initialize the classifier for the Sound chapter domain\n",
    "    selective_retrieval_agent = DomainClassifierTool(\"Sound Chapter (NCERT Class 9)\", relevant_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent for Web Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "class GoogleSearchClient:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.search_engine_id = os.getenv(\"SEARCH_ENGINE_ID\")\n",
    "        self.base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "\n",
    "    def search(self, query, num_results=3):\n",
    "        # Build the URL for the search query\n",
    "        encoded_query = quote_plus(query)\n",
    "        url = f\"{self.base_url}?key={self.api_key}&cx={self.search_engine_id}&q={encoded_query}&num={num_results}\"\n",
    "        \n",
    "        try:\n",
    "            # Send the request and return links from the results\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during search: {e}\")\n",
    "            return []\n",
    "\n",
    "        results = response.json().get('items', [])\n",
    "        return [item['link'] for item in results]\n",
    "\n",
    "class SearchManager:\n",
    "    def __init__(self, web_search_tool):\n",
    "        self.web_search_tool = web_search_tool\n",
    "\n",
    "    def get_search_results(self, query):\n",
    "        # Retrieve search results or return a fallback message\n",
    "        search_results = self.web_search_tool.search(query)\n",
    "        return search_results if search_results else \"No results found.\"\n",
    "\n",
    "# Initialize search client and manager\n",
    "api_key = os.getenv(\"CUSTOM_SEARCH_API_KEY\")\n",
    "web_search_tool = GoogleSearchClient(api_key)\n",
    "web_search_agent = SearchManager(web_search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.toppr.com/ask/question/does-sound-follow-the-same-laws-of-reflection-as-light/', 'https://byjus.com/question-answer/does-sound-follow-the-laws-of-reflection-as-light-does/', 'https://www.doubtnut.com/pcmb-questions/130851']\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"Does sound follow the same laws of reflection as light does? Explain\"\n",
    "print(web_search_agent.get_search_results(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text to Speech Service Integration using SarvamAI's API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "async def generate_speech(text: str) -> str:\n",
    "    \"\"\"Generate speech from text using the Sarvam API and save it as a WAV file.\"\"\"\n",
    "    url = \"https://api.sarvam.ai/text-to-speech\"\n",
    "\n",
    "    # Prepare the payload for the API request\n",
    "    payload = {\n",
    "        \"inputs\": [text],\n",
    "        \"target_language_code\": \"hi-IN\",\n",
    "        \"speaker\": \"meera\",\n",
    "        \"pitch\": 0,\n",
    "        \"pace\": 1.65,\n",
    "        \"loudness\": 1.5,\n",
    "        \"speech_sample_rate\": 8000,\n",
    "        \"enable_preprocessing\": True,\n",
    "        \"model\": \"bulbul:v1\"\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"api-subscription-key\": os.getenv(\"SARVAMAI_API_KEY\"),\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        audio_data = response.json().get('audios', [None])[0]\n",
    "        if audio_data:\n",
    "            file_path = 'output_audio.wav'\n",
    "            # Save the audio data to a file\n",
    "            with open(file_path, 'wb') as audio_file:\n",
    "                audio_file.write(base64.b64decode(audio_data))  \n",
    "            return file_path  \n",
    "        else:\n",
    "            raise Exception(\"No audio data found in the response\")\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FastAPI Endpoint - Navigate to http://localhost:8000/ or http://127.0.0.1:8000/ to access your application.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1436]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:61245 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:61245 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:61245 - \"POST /ask/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:61245 - \"GET /audio/output_audio.wav HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:61512 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:61520 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import FileResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware  # Import CORS middleware\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Configure CORS middleware to allow cross-origin requests\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  \n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  \n",
    "    allow_headers=[\"*\"],  \n",
    ")\n",
    "\n",
    "# Define the input model for the query\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "\n",
    "# Endpoint to handle question-asking\n",
    "@app.post(\"/ask/\")\n",
    "async def ask_rag(query: Query): \n",
    "    # Classify the query to check relevance\n",
    "    if selective_retrieval_agent.classify_query(query.question):\n",
    "        # Invoke the chain with the question\n",
    "        response = chain.invoke(query.question)\n",
    "        response += \"\\n\" + \"In response to your query, here are the links I found:\"\n",
    "\n",
    "        # Get related links from the web search agent\n",
    "        links = web_search_agent.get_search_results(query.question)\n",
    "\n",
    "        # Generate audio response for the text\n",
    "        audio_file_path = await generate_speech(response)\n",
    "\n",
    "        return {\n",
    "            \"response\": response,  \n",
    "            \"links\": links,       \n",
    "            \"audio_file\": audio_file_path  \n",
    "        }\n",
    "    else:\n",
    "        # If the query is not relevant\n",
    "        response = \"The query is not relevant to Chapter 11: Sound.\"\n",
    "        \n",
    "        # Generate audio response for the text\n",
    "        audio_file_path = await generate_speech(response)\n",
    "\n",
    "        return {\n",
    "            \"response\": response,  \n",
    "            \"audio_file\": audio_file_path  \n",
    "        }\n",
    "    \n",
    "# Endpoint to serve generated audio files\n",
    "@app.get(\"/audio/{file_name}\")\n",
    "def get_audio(file_name: str):\n",
    "    file_path = os.path.join(os.getcwd(), file_name)  # Construct file path\n",
    "    if os.path.exists(file_path):\n",
    "        return FileResponse(file_path)  # Serve the audio file\n",
    "    else:\n",
    "        raise HTTPException(status_code=404, detail=\"Audio file not found\")  # Return 404 if not found\n",
    "\n",
    "# Endpoint to serve the frontend HTML\n",
    "@app.get(\"/\")\n",
    "def serve_frontend():\n",
    "    return FileResponse(\"frontend.html\")  # Serve the frontend HTML file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)  # Configure Uvicorn server\n",
    "    server = uvicorn.Server(config)\n",
    "    \n",
    "    # Run the server\n",
    "    if not asyncio.get_event_loop().is_running():\n",
    "        server.run()\n",
    "    else:\n",
    "        asyncio.create_task(server.serve())  # Create a task to run the server if the loop is already running"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
